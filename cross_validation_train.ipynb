{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028b1fa9-04d3-4c44-8fbd-6e4a3e6028d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ONLY HAS TO BE RUN ONCE TO EXPORT DATASET FROM ZIP TO FOLDER\n",
    "import zipfile\n",
    "\n",
    "zip_path = \"Resources.zip\" \n",
    "extract_to = \"Dataset\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79cf06b9-6eaf-4014-b27d-fac97164f3cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'Dataset/training/patient001/Info.cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patient_path, file_name)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):  \u001b[38;5;66;03m# Check if it's a file\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Move the file to the respective patient folder in the group folder\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_patient_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# YOU CAN ONLY RUN THIS ONCE, AFTER THAT THE TRAINING SET IS EMPTY\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.11/shutil.py:571\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Recursively copy a directory tree and return the destination directory.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03mIf exception(s) occur, an Error is raised with a list of reasons.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m`src` tree.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutil.copytree\u001b[39m\u001b[38;5;124m\"\u001b[39m, src, dst)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[1;32m    572\u001b[0m     entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _copytree(entries\u001b[38;5;241m=\u001b[39mentries, src\u001b[38;5;241m=\u001b[39msrc, dst\u001b[38;5;241m=\u001b[39mdst, symlinks\u001b[38;5;241m=\u001b[39msymlinks,\n\u001b[1;32m    574\u001b[0m                  ignore\u001b[38;5;241m=\u001b[39mignore, copy_function\u001b[38;5;241m=\u001b[39mcopy_function,\n\u001b[1;32m    575\u001b[0m                  ignore_dangling_symlinks\u001b[38;5;241m=\u001b[39mignore_dangling_symlinks,\n\u001b[1;32m    576\u001b[0m                  dirs_exist_ok\u001b[38;5;241m=\u001b[39mdirs_exist_ok)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'Dataset/training/patient001/Info.cfg'"
     ]
    }
   ],
   "source": [
    "# ONLY HAS TO BE RUN ONCE TO ENABLE 5 FOLD CROSS VALIDATION\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Making a dictionary to store the disease class of each patient\n",
    "dataset_path = \"Dataset/training\"  # Path to train + validation folder\n",
    "patient_class_dict = {}\n",
    "\n",
    "for patient_folder in os.listdir(dataset_path): # For file in the training folder\n",
    "    if patient_folder.startswith(\".\"):  # Skip hidden folders (.ipynb_checkpoints)\n",
    "        continue\n",
    "\n",
    "    patient_path = os.path.join(dataset_path, patient_folder) # Patient ID, such as patient001\n",
    "\n",
    "    if os.path.isdir(patient_path):  # Process only valid patient folders (skip MANDATATORY_CITATION)\n",
    "        info_file = os.path.join(patient_path, \"Info.cfg\")\n",
    "        with open(info_file, \"r\") as patient_file: # Open file\n",
    "                lines = patient_file.readlines()\n",
    "                patient_class = lines[2].strip()\n",
    "                patient_class_dict[patient_folder] = patient_class # Add to dictionary\n",
    "        \n",
    "# Splitting dataset based on dictionary values (20 patients in each dataset)\n",
    "\n",
    "group_DCM = \"Dataset/group_DCM\"\n",
    "group_HCM = \"Dataset/group_HCM\"\n",
    "group_MINF = \"Dataset/group_MINF\"\n",
    "group_NOR = \"Dataset/group_NOR\"\n",
    "group_RV = \"Dataset/group_RV\"\n",
    "\n",
    "# Create the directories for each class\n",
    "for group in [group_DCM, group_HCM, group_MINF, group_NOR, group_RV]:\n",
    "    if not os.path.exists(group):\n",
    "        os.makedirs(group)\n",
    "\n",
    "# Loop through all patients\n",
    "for patient_folder, disease in patient_class_dict.items():\n",
    "    patient_path = os.path.join(dataset_path, patient_folder)\n",
    "\n",
    "    # Check if the folder exists and it's a directory\n",
    "    if os.path.isdir(patient_path):\n",
    "        # Determine the target group based on disease\n",
    "        if \"DCM\" in disease:\n",
    "            target_group = group_DCM\n",
    "        elif \"HCM\" in disease:\n",
    "            target_group = group_HCM\n",
    "        elif \"MINF\" in disease:\n",
    "            target_group = group_MINF\n",
    "        elif \"NOR\" in disease:\n",
    "            target_group = group_NOR\n",
    "        elif \"RV\" in disease:\n",
    "            target_group = group_RV\n",
    "        else:\n",
    "            print('unknown class error')\n",
    "            continue\n",
    "\n",
    "        # Create the patient's folder inside the target group directory\n",
    "        target_patient_folder = os.path.join(target_group, patient_folder)\n",
    "        if not os.path.exists(target_patient_folder):\n",
    "            os.makedirs(target_patient_folder)\n",
    "\n",
    "        # Copy respective files to new folder\n",
    "        for file_name in os.listdir(patient_path):\n",
    "            file_path = os.path.join(patient_path, file_name)\n",
    "            if os.path.isfile(file_path):  # Check if it's a file\n",
    "                # Move the file to the respective patient folder in the group folder\n",
    "                shutil.copytree(file_path, os.path.join(target_patient_folder, file_name)) # YOU CAN ONLY RUN THIS ONCE, AFTER THAT THE TRAINING SET IS EMPTY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57250931-bc04-45b8-9d33-be83f39cb578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:24:16.916115: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-10 11:24:20.239389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744277060.356549     234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744277060.415490     234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-10 11:24:20.602083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Importing!\n"
     ]
    }
   ],
   "source": [
    "## ALL FUNCTION NEEDED TO TRAIN AND VALIDATE MODEL (Has to be run before any code segment)\n",
    "import os\n",
    "import shutil\n",
    "data_path_train = \"Dataset/new_training\"\n",
    "data_path_valid = \"Dataset/new_validation\"\n",
    "data_path_test = \"Dataset/testing\"\n",
    "\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import monai\n",
    "import torch.nn.functional as F\n",
    "from medpy.metric.binary import hd, dc\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    LoadImaged,         # if using file paths\n",
    "    AddChanneld,        # ensures channel-first format\n",
    "    ScaleIntensityd,    # normalizes intensity to [minv, maxv]\n",
    "    Spacingd,           # resamples to a uniform voxel spacing\n",
    "    ResizeWithPadOrCropd,  # resizes images/masks to a fixed spatial size\n",
    "    EnsureTyped,        # converts arrays to PyTorch tensors\n",
    "    RandZoomd,          # random zoom augmentation\n",
    "    RandFlipd,          # random flip augmentation\n",
    "    RandRotated,        # random rotation augmentation\n",
    "    RandShiftIntensityd # random intensity shift for brightness variation\n",
    ")\n",
    "\n",
    "# Start up wandb and start logging\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done Importing!\")\n",
    "\n",
    "def get_ed_es_frames(config_path):\n",
    "    \"\"\"Extract ED and ES frame numbers from the info.cfg file.\"\"\"\n",
    "\n",
    "    ed_frame, es_frame = None, None\n",
    "    with open(config_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('ED:'):\n",
    "                ed_frame = int(line.split(':')[1].strip())\n",
    "            elif line.startswith('ES:'):\n",
    "                es_frame = int(line.split(':')[1].strip())\n",
    "    return ed_frame, es_frame\n",
    "\n",
    "\n",
    "def build_dict_acdc(data_path, mode='train'):\n",
    "    \"\"\"\n",
    "    This function returns a list of dictionaries, each containing the paths to the 2D slices \n",
    "    of the 3D MRI images and their corresponding masks.\n",
    "    \"\"\"\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n",
    "    \n",
    "    dicts = []\n",
    "    \n",
    "    # Loop over all patient directories\n",
    "    patient_dirs = [d for d in glob.glob(os.path.join(data_path, '*')) if os.path.isdir(d)]\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        patient_id = os.path.basename(patient_dir)\n",
    "        config_path = os.path.join(patient_dir, \"Info.cfg\")\n",
    "        \n",
    "        if not os.path.exists(config_path):\n",
    "            continue\n",
    "        \n",
    "        ed_frame, es_frame = get_ed_es_frames(config_path)\n",
    "        \n",
    "        # Identify the ED and ES image and mask paths\n",
    "        ed_img_path = os.path.join(patient_dir, f\"{patient_id}_frame{ed_frame:02d}.nii.gz\")\n",
    "        ed_mask_path = os.path.join(patient_dir, f\"{patient_id}_frame{ed_frame:02d}_gt.nii.gz\")\n",
    "        es_img_path = os.path.join(patient_dir, f\"{patient_id}_frame{es_frame:02d}.nii.gz\")\n",
    "        es_mask_path = os.path.join(patient_dir, f\"{patient_id}_frame{es_frame:02d}_gt.nii.gz\")\n",
    "        \n",
    "        for img_path, mask_path in [(ed_img_path, ed_mask_path), (es_img_path, es_mask_path)]:\n",
    "            if not os.path.exists(img_path) or not os.path.exists(mask_path):\n",
    "                continue\n",
    "            \n",
    "            # Load the 3D image and mask using nibabel\n",
    "            img_volume = nib.load(img_path).get_fdata()\n",
    "            mask_volume = nib.load(mask_path).get_fdata()\n",
    "            #print(\"Unique values in loaded ground truth mask:\", np.unique(mask_volume))\n",
    "            \n",
    "            # Ensure we have the same number of slices for image and mask\n",
    "            num_slices = img_volume.shape[2]\n",
    "            \n",
    "            # Extract 2D slices\n",
    "            for slice_idx in range(num_slices):\n",
    "                img_slice = img_volume[:, :, slice_idx]\n",
    "                mask_slice = mask_volume[:, :, slice_idx]\n",
    "                \n",
    "                dicts.append({'img': img_slice, 'mask': mask_slice})\n",
    "    \n",
    "    return dicts\n",
    "\n",
    "class LoadHeartData(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads 2D slices of MRI data and their corresponding mask for heart segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img_slice = sample['img']\n",
    "        mask_slice = sample['mask'] \n",
    "        \n",
    "        # Ensure the image and mask are in compatible formats\n",
    "        img_slice = np.array(img_slice, dtype=np.float32)\n",
    "        mask_slice = np.array(mask_slice, dtype=np.uint8) \n",
    "        \n",
    "        # Return the slice and mask with metadata. NOT SURE ABOUT THE METATDATA\n",
    "        return {'img': img_slice, 'mask': mask_slice, 'img_meta_dict': {'affine': np.eye(2)}, \n",
    "                'mask_meta_dict': {'affine': np.eye(2)}}\n",
    "\n",
    "HEADER = [\"Name\", \"Dice LV\", \"Volume LV\", \"Err LV(ml)\",\n",
    "          \"Dice RV\", \"Volume RV\", \"Err RV(ml)\",\n",
    "          \"Dice MYO\", \"Volume MYO\", \"Err MYO(ml)\"]\n",
    "\n",
    "#\n",
    "# Functions to process files, directories and metrics aka loss function\n",
    "#\n",
    "def metrics(img_gt, img_pred, voxel_size):\n",
    "    \"\"\"\n",
    "    Function to compute the metrics between two segmentation maps given as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_gt: np.array\n",
    "    Array of the ground truth segmentation map.\n",
    "\n",
    "    img_pred: np.array\n",
    "    Array of the predicted segmentation map.\n",
    "\n",
    "    voxel_size: list, tuple or np.array\n",
    "    The size of a voxel of the images used to compute the volumes.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of metrics in this order, [Dice LV, Volume LV, Err LV(ml),\n",
    "    Dice RV, Volume RV, Err RV(ml), Dice MYO, Volume MYO, Err MYO(ml)]\n",
    "    \"\"\"\n",
    "\n",
    "    if img_gt.ndim != img_pred.ndim:\n",
    "        raise ValueError(\"The arrays 'img_gt' and 'img_pred' should have the \"\n",
    "                         \"same dimension, {} against {}\".format(img_gt.ndim,\n",
    "                                                                img_pred.ndim))\n",
    "    #print(\"Unique values in ground truth:\", np.unique(img_gt))\n",
    "    #print(\"Unique values in prediction:\", np.unique(img_pred))\n",
    "    \n",
    "    res = []\n",
    "    # Loop on each classes of the input images\n",
    "    for c in [3, 1, 2]:\n",
    "        # Copy the gt image to not alterate the input\n",
    "        gt_c_i = np.copy(img_gt)\n",
    "        gt_c_i[gt_c_i != c] = 0\n",
    "\n",
    "        # Copy the pred image to not alterate the input\n",
    "        pred_c_i = np.copy(img_pred)\n",
    "        pred_c_i[pred_c_i != c] = 0\n",
    "\n",
    "        # Clip the value to compute the volumes\n",
    "        gt_c_i = np.clip(gt_c_i, 0, 1)\n",
    "        pred_c_i = np.clip(pred_c_i, 0, 1)\n",
    "\n",
    "        # Compute the Dice\n",
    "        dice = dc(gt_c_i, pred_c_i)\n",
    "\n",
    "        # Compute volume\n",
    "        volpred = pred_c_i.sum() * np.prod(voxel_size) / 1000.\n",
    "        volgt = gt_c_i.sum() * np.prod(voxel_size) / 1000.\n",
    "\n",
    "        res += [dice, volpred, volpred-volgt]\n",
    "\n",
    "    return res\n",
    "\n",
    "def compute_metrics_on_files(path_gt, path_pred):\n",
    "    \"\"\"\n",
    "    Function to give the metrics for two files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    path_gt: string\n",
    "    Path of the ground truth image.\n",
    "\n",
    "    path_pred: string\n",
    "    Path of the predicted image.\n",
    "    \"\"\"\n",
    "    gt, _, header = load_nii(path_gt)\n",
    "    pred, _, _ = load_nii(path_pred)\n",
    "    zooms = header.get_zooms()\n",
    "\n",
    "    name = os.path.basename(path_gt)\n",
    "    name = name.split('.')[0]\n",
    "    res = metrics(gt, pred, zooms)\n",
    "    res = [\"{:.3f}\".format(r) for r in res]\n",
    "\n",
    "    formatting = \"{:>14}, {:>7}, {:>9}, {:>10}, {:>7}, {:>9}, {:>10}, {:>8}, {:>10}, {:>11}\"\n",
    "    print(formatting.format(*HEADER))\n",
    "    print(formatting.format(name, *res))\n",
    "\n",
    "    \n",
    "# Note that this is a clear, but slow way to do this, we might be better off with a quick hardcode since the patients are ordered anyways.\n",
    "# So technically, the first 20 of train is just group DCM and we dont need to make any folders or copy any files.\n",
    "# this could also be sped up by moving per patient and not per file per patient (the last loop)\n",
    "# DOOR ERIC        \n",
    "# Recombine into a training and a validation set (set 1 to validation and 4 to training)\n",
    "def recombining_data(recombine_index):\n",
    "    \n",
    "    new_train_path = os.path.join(\"Dataset\", 'new_training')\n",
    "    new_val_path = os.path.join(\"Dataset\", 'new_validation')\n",
    "    \n",
    "    # If folder does not exist yet\n",
    "    if not os.path.exists(new_train_path):\n",
    "        os.makedirs(new_train_path)  # Creates the new training folder\n",
    "    if not os.path.exists(new_val_path):\n",
    "        os.makedirs(new_val_path)  # Creates the new validation folder\n",
    "    \n",
    "    \n",
    "    # Empty the new_validation folder\n",
    "    if os.path.exists(new_val_path):\n",
    "        shutil.rmtree(new_val_path)\n",
    "        os.makedirs(new_val_path)\n",
    "\n",
    "    # Empty the new_training folder\n",
    "    if os.path.exists(new_train_path):\n",
    "        shutil.rmtree(new_train_path)\n",
    "        os.makedirs(new_train_path)\n",
    "    \n",
    "   \n",
    "    val_id = [1,2,3,4]\n",
    "    offset = (recombine_index - 1) * 4\n",
    "    val_id = [element + offset for element in val_id] # Add the offset to each element of val_id\n",
    "    #print(val_id)\n",
    "    \n",
    "    train_id = list(range(1,21))\n",
    "    for element in val_id:\n",
    "        train_id.remove(element) # remove the validation patients\n",
    "    #print(train_id)\n",
    "    \n",
    "    # now there is a list of numbers for who should be in val, and who should be in train\n",
    "    \n",
    "    # Define the classes (group folders) you want to loop through\n",
    "    class_folders = ['group_DCM', 'group_HCM', 'group_MINF', 'group_NOR', 'group_RV']\n",
    "    \n",
    "    # Loop through each class folder\n",
    "    for class_folder in class_folders:\n",
    "        class_folder_path = os.path.join(\"Dataset\", class_folder)\n",
    "        \n",
    "        patients_in_class = [folder for folder in os.listdir(class_folder_path)] # list of all file names in class\n",
    "        \n",
    "        for val_target in val_id: # copy all validation patients\n",
    "            val_patient_target = patients_in_class[val_target - 1] # get name of validation patient\n",
    "            #print(f\"Processing validation patient: {val_patient_target}\")\n",
    "            \n",
    "            # copy from source to destination\n",
    "            source_folder = os.path.join(\"Dataset\", class_folder,val_patient_target)\n",
    "            destination_folder = os.path.join(new_val_path, val_patient_target)\n",
    "            shutil.copytree(source_folder, destination_folder)\n",
    "            \n",
    "        for train_target in train_id: # copy all training patients\n",
    "            train_patient_target = patients_in_class[train_target - 1] # get name of training patient\n",
    "            #print(f\"Processing training patient: {train_patient_target}\")\n",
    "            \n",
    "            # copy from source to destination\n",
    "            source_folder = os.path.join(\"Dataset\", class_folder,train_patient_target)\n",
    "            destination_folder = os.path.join(new_train_path, train_patient_target)\n",
    "            shutil.copytree(source_folder, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd65ec1f-f089-48b6-90c5-6bbc14d39368",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Importing!\n",
      "Beginning the loop\n",
      "Beginning loop for index:1\n",
      "Recombining Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdejanhonderd100\u001b[0m (\u001b[33mDLMI_Project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DLMIA_Project/DLMIA/wandb/run-20250325_130018-l6l7fh7n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/l6l7fh7n' target=\"_blank\">PreProc_BasicUNet_Cross_validation_run_1</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/l6l7fh7n' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/l6l7fh7n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1506/1506 [00:15<00:00, 94.53it/s] \n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 90.24it/s]\n",
      "Loading dataset: 100%|██████████| 396/396 [00:03<00:00, 106.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.5.1+cu121\n",
      "---------- Epoch 1/10 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/torch/_tensor.py:1512: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/torch/_tensor.py:1512: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/torch/_tensor.py:1512: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/torch/_tensor.py:1512: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/monai/data/__init__.py:118: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.8320, time: 25.61 sec\n",
      "Validation metrics: [ 0.12406754  5.70952888  5.12298769  0.22892039  3.87763573  3.32714646\n",
      "  0.02943132 22.41736111 21.80479403]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7532, time: 13.78 sec\n",
      "Validation metrics: [ 0.06869822 11.93392914 11.34738794  0.36016891  1.89311869  1.34262942\n",
      "  0.39665709  1.75598169  1.14341461]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6579, time: 14.67 sec\n",
      "Validation metrics: [0.08060192 9.5943971  9.0078559  0.49549249 0.97700836 0.4265191\n",
      " 0.57933956 1.11699416 0.50442708]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.5428, time: 13.46 sec\n",
      "Validation metrics: [ 0.66214257  0.53847064 -0.04807055  0.51872242  0.88786695  0.33737768\n",
      "  0.63221529  0.76912879  0.15656171]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.3845, time: 13.65 sec\n",
      "Validation metrics: [0.73582685 0.59107876 0.00453756 0.55744761 0.76214489 0.21165562\n",
      " 0.65240133 0.63923217 0.02666509]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.3018, time: 13.56 sec\n",
      "Validation metrics: [ 0.73185636  0.58112374 -0.00541746  0.54703137  0.64571891  0.09522964\n",
      "  0.64844679  0.59760101 -0.01496607]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.2574, time: 13.68 sec\n",
      "Validation metrics: [ 0.77157176  0.56445707 -0.02208412  0.56453148  0.56527778  0.01478851\n",
      "  0.65536755  0.5831834  -0.02938368]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.2300, time: 13.44 sec\n",
      "Validation metrics: [ 0.78481073  0.55222932 -0.03431187  0.58984686  0.65964331  0.10915404\n",
      "  0.6819368   0.59069208 -0.021875  ]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.2129, time: 13.68 sec\n",
      "Validation metrics: [ 0.78417449  0.54987374 -0.03666746  0.57447365  0.52931266 -0.02117661\n",
      "  0.66942622  0.58665956 -0.02590751]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.2013, time: 13.58 sec\n",
      "Validation metrics: [ 0.79790691  0.56481218 -0.02172901  0.58333294  0.55688526  0.00639599\n",
      "  0.69313893  0.60901989 -0.00354719]\n",
      "Model saved at PreProc_BasicUNet_Cross_validation_models/PreProc_BasicUNet_Cross_validation_cross_variant_1.pth\n",
      "Validation metrics: [ 0.763752    0.59488993 -0.02089045  0.58243288  0.61540863 -0.02565346\n",
      "  0.65449416  0.65865764  0.05734346]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 1.0%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▂▁▁▇▇▇████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▅▇▇██████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▄▆▇▇▇████</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>▄█▇▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>▄█▇▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_loss</td><td>█▇▆▅▃▂▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>█▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_step_loss</td><td>█████▇▇▇▆▆▆▆▅▆▅▅▄▃▄▃▃▄▂▂▂▄▃▃▃▁▃▂▂▁▁▃▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.79791</td></tr><tr><td>Dice_LV_test</td><td>0.76375</td></tr><tr><td>Dice_MY0</td><td>0.69314</td></tr><tr><td>Dice_MY0_test</td><td>0.65449</td></tr><tr><td>Dice_RV</td><td>0.58333</td></tr><tr><td>Dice_RV_test</td><td>0.58243</td></tr><tr><td>Err_LV</td><td>-0.02173</td></tr><tr><td>Err_LV_test</td><td>-0.02089</td></tr><tr><td>Err_MY0</td><td>-0.00355</td></tr><tr><td>Err_MY0_test</td><td>0.05734</td></tr><tr><td>Err_RV</td><td>0.0064</td></tr><tr><td>Err_RV_test</td><td>-0.02565</td></tr><tr><td>Volume_LV</td><td>0.56481</td></tr><tr><td>Volume_LV_test</td><td>0.59489</td></tr><tr><td>Volume_MY0</td><td>0.60902</td></tr><tr><td>Volume_MY0_test</td><td>0.65866</td></tr><tr><td>Volume_RV</td><td>0.55689</td></tr><tr><td>Volume_RV_test</td><td>0.61541</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.20131</td></tr><tr><td>epoch_time_sec</td><td>13.58139</td></tr><tr><td>train_step_loss</td><td>0.09801</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_BasicUNet_Cross_validation_run_1</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/l6l7fh7n' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/l6l7fh7n</a><br/> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_130018-l6l7fh7n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:2\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DLMIA_Project/DLMIA/wandb/run-20250325_130426-nzc74v3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/nzc74v3o' target=\"_blank\">PreProc_BasicUNet_Cross_validation_run_2</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/nzc74v3o' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/nzc74v3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1514/1514 [00:16<00:00, 93.96it/s] \n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 97.07it/s]\n",
      "Loading dataset: 100%|██████████| 388/388 [00:03<00:00, 105.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.5.1+cu121\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8516, time: 14.38 sec\n",
      "Validation metrics: [ 0.02717469 19.97388048 19.2990174   0.21607139  4.96375644  4.26749356\n",
      "  0.0623662   8.19134182  7.47769008]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7924, time: 14.09 sec\n",
      "Validation metrics: [ 0.0204755  20.35251289 19.67764981  0.31729939  2.43342059  1.7371577\n",
      "  0.25664161  4.0071279   3.29347616]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.7355, time: 14.21 sec\n",
      "Validation metrics: [ 0.02265566 15.93504349 15.26018041  0.4393896   1.57000242  0.87373953\n",
      "  0.36957038  2.51162613  1.79797439]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.6747, time: 13.97 sec\n",
      "Validation metrics: [ 0.38969708  0.65990657 -0.01495651  0.48158847  1.2123067   0.51604381\n",
      "  0.45368092  1.73048083  1.01682909]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.5816, time: 13.36 sec\n",
      "Validation metrics: [ 0.48737669  0.37333682 -0.30152626  0.53138632  0.95369684  0.25743396\n",
      "  0.53678008  1.26458199  0.55093025]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.4651, time: 14.12 sec\n",
      "Validation metrics: [ 0.52043852  0.3540472  -0.32081588  0.52644136  0.69939997  0.00313708\n",
      "  0.62633498  0.82040915  0.10675741]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.3664, time: 13.89 sec\n",
      "Validation metrics: [ 0.52914271  0.35137726 -0.32348582  0.5489076   0.67479462 -0.02146827\n",
      "  0.67243336  0.81903592  0.10538418]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.3143, time: 14.08 sec\n",
      "Validation metrics: [ 0.53664771  0.32080783 -0.35405525  0.57795588  0.72624034  0.02997745\n",
      "  0.67818937  0.75212629  0.03847455]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.2903, time: 12.58 sec\n",
      "Validation metrics: [ 0.53636136  0.34256604 -0.33229704  0.56349521  0.64728173 -0.04898115\n",
      "  0.67971818  0.71019652 -0.00345522]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.2737, time: 13.58 sec\n",
      "Validation metrics: [ 0.53603629  0.31928963 -0.35557345  0.56514167  0.73214401  0.03588112\n",
      "  0.67953795  0.73640464  0.0227529 ]\n",
      "Model saved at PreProc_BasicUNet_Cross_validation_models/PreProc_BasicUNet_Cross_validation_cross_variant_2.pth\n",
      "Validation metrics: [ 0.51407063  0.30634294 -0.30943744  0.60455593  0.67079606  0.02973397\n",
      "  0.66446819  0.66133393  0.06001975]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▁▁▆▇█████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▃▄▅▆▇████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>██▆▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>██▆▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_loss</td><td>█▇▇▆▅▃▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>█▇▇▆▄▇▆▇▁▅</td></tr><tr><td>train_step_loss</td><td>██▇▇█▇▇▇▆▇▆▆▆▆▆▆▅▅▅▄▅▄▃▄▄▂▃▃▂▂▁▃▃▁▃▂▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.53604</td></tr><tr><td>Dice_LV_test</td><td>0.51407</td></tr><tr><td>Dice_MY0</td><td>0.67954</td></tr><tr><td>Dice_MY0_test</td><td>0.66447</td></tr><tr><td>Dice_RV</td><td>0.56514</td></tr><tr><td>Dice_RV_test</td><td>0.60456</td></tr><tr><td>Err_LV</td><td>-0.35557</td></tr><tr><td>Err_LV_test</td><td>-0.30944</td></tr><tr><td>Err_MY0</td><td>0.02275</td></tr><tr><td>Err_MY0_test</td><td>0.06002</td></tr><tr><td>Err_RV</td><td>0.03588</td></tr><tr><td>Err_RV_test</td><td>0.02973</td></tr><tr><td>Volume_LV</td><td>0.31929</td></tr><tr><td>Volume_LV_test</td><td>0.30634</td></tr><tr><td>Volume_MY0</td><td>0.7364</td></tr><tr><td>Volume_MY0_test</td><td>0.66133</td></tr><tr><td>Volume_RV</td><td>0.73214</td></tr><tr><td>Volume_RV_test</td><td>0.6708</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.27372</td></tr><tr><td>epoch_time_sec</td><td>13.58214</td></tr><tr><td>train_step_loss</td><td>0.22724</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_BasicUNet_Cross_validation_run_2</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/nzc74v3o' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/nzc74v3o</a><br/> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_130426-nzc74v3o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:3\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DLMIA_Project/DLMIA/wandb/run-20250325_130834-y9o67d2t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/y9o67d2t' target=\"_blank\">PreProc_BasicUNet_Cross_validation_run_3</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/y9o67d2t' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/y9o67d2t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1538/1538 [00:17<00:00, 87.02it/s]\n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 95.61it/s]\n",
      "Loading dataset: 100%|██████████| 364/364 [00:03<00:00, 100.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.5.1+cu121\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8152, time: 15.31 sec\n",
      "Validation metrics: [0.15166227 6.2986693  5.64081817 0.02106528 8.10744763 7.58049451\n",
      " 0.29508019 2.14590917 1.50621995]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7316, time: 15.81 sec\n",
      "Validation metrics: [0.63067492 0.84399038 0.18613925 0.08656145 6.8160285  6.28907538\n",
      " 0.51824934 1.297858   0.65816878]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6221, time: 15.64 sec\n",
      "Validation metrics: [0.7158785  0.7091432  0.05129207 0.33767949 1.43497167 0.90801854\n",
      " 0.60624328 0.87189217 0.23220295]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.4756, time: 15.27 sec\n",
      "Validation metrics: [ 0.74596707  0.60605254 -0.05179859  0.50696621  0.79362552  0.26667239\n",
      "  0.63439662  0.7332203   0.09353108]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.3592, time: 14.39 sec\n",
      "Validation metrics: [ 0.752544    0.64827009 -0.00958104  0.52096863  0.7468149   0.21986178\n",
      "  0.64406679  0.70277301  0.06308379]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.2950, time: 15.66 sec\n",
      "Validation metrics: [7.72501229e-01 6.58598043e-01 7.46909341e-04 5.36492691e-01\n",
      " 7.25879979e-01 1.98926854e-01 6.54933333e-01 6.98411745e-01\n",
      " 5.87225275e-02]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.2518, time: 15.84 sec\n",
      "Validation metrics: [0.77344557 0.67902644 0.02117531 0.54460695 0.72031679 0.19336367\n",
      " 0.6739258  0.69214887 0.05245965]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.2290, time: 14.56 sec\n",
      "Validation metrics: [ 0.77311588  0.65562758 -0.00222356  0.54068398  0.86856542  0.34161229\n",
      "  0.68488764  0.6511547   0.01146549]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.2148, time: 14.68 sec\n",
      "Validation metrics: [0.77568687 0.66176168 0.00391054 0.56084695 0.68888221 0.16192909\n",
      " 0.68968024 0.68934152 0.0496523 ]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.1981, time: 15.20 sec\n",
      "Validation metrics: [ 0.78611879  0.69522236  0.03737122  0.57441058  0.59697373  0.0700206\n",
      "  0.68726792  0.63421188 -0.00547734]\n",
      "Model saved at PreProc_BasicUNet_Cross_validation_models/PreProc_BasicUNet_Cross_validation_cross_variant_3.pth\n",
      "Validation metrics: [ 0.78339906  0.63103363  0.01525325  0.59993665  0.65016554  0.00910345\n",
      "  0.66390994  0.58352405 -0.01779014]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▆▇███████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▅▇▇▇▇████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▂▅▇▇█████</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▄▂▁▁▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>█▇▂▁▁▁▁▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▄▂▁▁▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>█▇▂▁▁▁▁▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_loss</td><td>█▇▆▄▃▂▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>▅█▇▅▁▇█▂▂▅</td></tr><tr><td>train_step_loss</td><td>███▇▇▇▇▇▆▆▅▆▄▅▅▄▅▂▄▃▃▂▄▂▃▂▂▁▂▂▂▂▂▁▂▁▂▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.78612</td></tr><tr><td>Dice_LV_test</td><td>0.7834</td></tr><tr><td>Dice_MY0</td><td>0.68727</td></tr><tr><td>Dice_MY0_test</td><td>0.66391</td></tr><tr><td>Dice_RV</td><td>0.57441</td></tr><tr><td>Dice_RV_test</td><td>0.59994</td></tr><tr><td>Err_LV</td><td>0.03737</td></tr><tr><td>Err_LV_test</td><td>0.01525</td></tr><tr><td>Err_MY0</td><td>-0.00548</td></tr><tr><td>Err_MY0_test</td><td>-0.01779</td></tr><tr><td>Err_RV</td><td>0.07002</td></tr><tr><td>Err_RV_test</td><td>0.0091</td></tr><tr><td>Volume_LV</td><td>0.69522</td></tr><tr><td>Volume_LV_test</td><td>0.63103</td></tr><tr><td>Volume_MY0</td><td>0.63421</td></tr><tr><td>Volume_MY0_test</td><td>0.58352</td></tr><tr><td>Volume_RV</td><td>0.59697</td></tr><tr><td>Volume_RV_test</td><td>0.65017</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.19811</td></tr><tr><td>epoch_time_sec</td><td>15.19553</td></tr><tr><td>train_step_loss</td><td>0.0877</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_BasicUNet_Cross_validation_run_3</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/y9o67d2t' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/y9o67d2t</a><br/> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_130834-y9o67d2t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:4\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DLMIA_Project/DLMIA/wandb/run-20250325_131307-hx47p4pu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/hx47p4pu' target=\"_blank\">PreProc_BasicUNet_Cross_validation_run_4</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/hx47p4pu' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/hx47p4pu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1498/1498 [00:16<00:00, 88.98it/s] \n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 95.82it/s]\n",
      "Loading dataset: 100%|██████████| 404/404 [00:04<00:00, 90.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.5.1+cu121\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8364, time: 14.62 sec\n",
      "Validation metrics: [ 0.1880674   1.55946782  0.87210319  0.03545438 14.29804301 13.6268603\n",
      "  0.26375692  3.55461015  2.90492729]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7651, time: 15.53 sec\n",
      "Validation metrics: [ 0.4221296   0.45799428 -0.22937036  0.02948592 15.74422184 15.07303914\n",
      "  0.40378781  2.14863861  1.49895575]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6999, time: 15.23 sec\n",
      "Validation metrics: [ 0.48521923  0.37824103 -0.30912361  0.03539615 15.37170869 14.70052599\n",
      "  0.48001632  1.50236309  0.85268023]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.6246, time: 15.23 sec\n",
      "Validation metrics: [ 0.49489173  0.33582534 -0.35153929  0.03604384 15.69794632 15.02676361\n",
      "  0.52711107  1.12549892  0.47581606]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.5584, time: 15.19 sec\n",
      "Validation metrics: [ 0.50327283  0.35818765 -0.32917698  0.03802222 14.4264078  13.75522509\n",
      "  0.53921129  1.19067915  0.54099629]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.5168, time: 15.81 sec\n",
      "Validation metrics: [ 0.5310518   0.36468131 -0.32268332  0.04300314 13.57935102 12.90816832\n",
      "  0.5708801   1.05877553  0.40909267]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.4908, time: 15.12 sec\n",
      "Validation metrics: [ 0.51683795  0.35528311 -0.33208153  0.05137921 10.61247679  9.94129409\n",
      "  0.55683172  1.08351253  0.43382967]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.4381, time: 15.55 sec\n",
      "Validation metrics: [ 0.52361746  0.33776686 -0.34959777  0.35380147  0.44556389 -0.22561881\n",
      "  0.56910195  1.08369431  0.43401145]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.3802, time: 15.25 sec\n",
      "Validation metrics: [ 0.53630059  0.34469369 -0.34267095  0.39458159  0.44681699 -0.22436572\n",
      "  0.58203777  0.99464341  0.34496055]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.3607, time: 15.22 sec\n",
      "Validation metrics: [ 0.5228984   0.32825263 -0.359112    0.39952913  0.37194462 -0.29923809\n",
      "  0.57816187  1.06204749  0.41236463]\n",
      "Model saved at PreProc_BasicUNet_Cross_validation_models/PreProc_BasicUNet_Cross_validation_cross_variant_4.pth\n",
      "Validation metrics: [ 0.50241571  0.30433608 -0.3114443   0.39033667  0.37924605 -0.26181604\n",
      "  0.5527351   1.02184741  0.42053322]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▆▇▇▇█████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▄▆▇▇█▇███</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▁▁▁▁▁▁▇██</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▄▂▁▂▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>▇███▇▇▆▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▄▂▁▂▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>▇███▇▇▆▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_loss</td><td>█▇▆▅▄▃▃▂▁▁</td></tr><tr><td>epoch_time_sec</td><td>▁▆▅▅▄█▄▇▅▅</td></tr><tr><td>train_step_loss</td><td>██▇▇▇▇▇▆▆▆▆▅▆▄▅▅▃▄▄▃▃▃▄▃▃▃▃▄▃▂▂▂▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.5229</td></tr><tr><td>Dice_LV_test</td><td>0.50242</td></tr><tr><td>Dice_MY0</td><td>0.57816</td></tr><tr><td>Dice_MY0_test</td><td>0.55274</td></tr><tr><td>Dice_RV</td><td>0.39953</td></tr><tr><td>Dice_RV_test</td><td>0.39034</td></tr><tr><td>Err_LV</td><td>-0.35911</td></tr><tr><td>Err_LV_test</td><td>-0.31144</td></tr><tr><td>Err_MY0</td><td>0.41236</td></tr><tr><td>Err_MY0_test</td><td>0.42053</td></tr><tr><td>Err_RV</td><td>-0.29924</td></tr><tr><td>Err_RV_test</td><td>-0.26182</td></tr><tr><td>Volume_LV</td><td>0.32825</td></tr><tr><td>Volume_LV_test</td><td>0.30434</td></tr><tr><td>Volume_MY0</td><td>1.06205</td></tr><tr><td>Volume_MY0_test</td><td>1.02185</td></tr><tr><td>Volume_RV</td><td>0.37194</td></tr><tr><td>Volume_RV_test</td><td>0.37925</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.36066</td></tr><tr><td>epoch_time_sec</td><td>15.21677</td></tr><tr><td>train_step_loss</td><td>0.53513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_BasicUNet_Cross_validation_run_4</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/hx47p4pu' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/hx47p4pu</a><br/> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_131307-hx47p4pu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:5\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DLMIA_Project/DLMIA/wandb/run-20250325_131724-vwi24cdd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/vwi24cdd' target=\"_blank\">PreProc_BasicUNet_Cross_validation_run_5</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/vwi24cdd' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/vwi24cdd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/new_python_3_11_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1552/1552 [00:17<00:00, 90.56it/s] \n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 96.59it/s]\n",
      "Loading dataset: 100%|██████████| 350/350 [00:03<00:00, 104.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA version (PyTorch): 12.1\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.5.1+cu121\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8404, time: 15.65 sec\n",
      "Validation metrics: [ 0.40698449  2.33869196  1.51647321  0.03188176 18.55836161 17.74334821\n",
      "  0.0255951  21.89632143 21.00029464]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7808, time: 14.87 sec\n",
      "Validation metrics: [ 0.61117535  1.1174375   0.29521875  0.0360621  18.65180804 17.83679464\n",
      "  0.1643364   3.09306696  2.19704018]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.7134, time: 12.55 sec\n",
      "Validation metrics: [ 0.69163608  0.76673214 -0.05548661  0.03103127 16.36521429 15.55020089\n",
      "  0.45345889  1.23683482  0.34080804]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.6183, time: 15.05 sec\n",
      "Validation metrics: [ 0.71273157  0.83241071  0.01019196  0.29354598  0.49544196 -0.31957143\n",
      "  0.52978295  1.1378125   0.24178571]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.4905, time: 15.02 sec\n",
      "Validation metrics: [ 0.73390497  0.90064732  0.07842857  0.35095891  0.48403125 -0.33098214\n",
      "  0.5999645   0.78062054 -0.11540625]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.3829, time: 15.24 sec\n",
      "Validation metrics: [ 0.76471103  0.68415625 -0.1380625   0.38874912  0.48198214 -0.33303125\n",
      "  0.63709999  0.85805804 -0.03796875]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.3209, time: 15.22 sec\n",
      "Validation metrics: [ 0.77106566  0.73947768 -0.08274107  0.38055256  0.39721875 -0.41779464\n",
      "  0.64269182  0.80184375 -0.09418304]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.2908, time: 15.09 sec\n",
      "Validation metrics: [ 0.76272414  0.60897768 -0.21324107  0.396952    0.46350446 -0.35150893\n",
      "  0.65470046  0.70761607 -0.18841071]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.2713, time: 14.69 sec\n",
      "Validation metrics: [ 0.77560182  0.66852232 -0.15369643  0.38793603  0.4055     -0.40951339\n",
      "  0.6744713   0.80266071 -0.09336607]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.2579, time: 14.98 sec\n",
      "Validation metrics: [ 0.77790204  0.63183036 -0.19038839  0.39637686  0.41975893 -0.39525446\n",
      "  0.67386638  0.73411161 -0.16191518]\n",
      "Model saved at PreProc_BasicUNet_Cross_validation_models/PreProc_BasicUNet_Cross_validation_cross_variant_5.pth\n",
      "Validation metrics: [ 0.75041559  0.5533617  -0.06241868  0.39778278  0.42338668 -0.21767542\n",
      "  0.64507311  0.61045684  0.00914266]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▅▆▇▇█████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▂▆▆▇█████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▁▁▆▇█████</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▃▂▂▂▁▂▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>██▇▁▁▁▁▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▃▂▂▂▁▂▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>██▇▁▁▁▁▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_loss</td><td>█▇▆▅▄▃▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>█▆▁▇▇▇▇▇▆▆</td></tr><tr><td>train_step_loss</td><td>████▇▇▇▇▆▇▆▆▆▆▆▅▆▄▄▄▂▃▃▄▄▄▂▂▂▂▁▁▂▁▁▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.7779</td></tr><tr><td>Dice_LV_test</td><td>0.75042</td></tr><tr><td>Dice_MY0</td><td>0.67387</td></tr><tr><td>Dice_MY0_test</td><td>0.64507</td></tr><tr><td>Dice_RV</td><td>0.39638</td></tr><tr><td>Dice_RV_test</td><td>0.39778</td></tr><tr><td>Err_LV</td><td>-0.19039</td></tr><tr><td>Err_LV_test</td><td>-0.06242</td></tr><tr><td>Err_MY0</td><td>-0.16192</td></tr><tr><td>Err_MY0_test</td><td>0.00914</td></tr><tr><td>Err_RV</td><td>-0.39525</td></tr><tr><td>Err_RV_test</td><td>-0.21768</td></tr><tr><td>Volume_LV</td><td>0.63183</td></tr><tr><td>Volume_LV_test</td><td>0.55336</td></tr><tr><td>Volume_MY0</td><td>0.73411</td></tr><tr><td>Volume_MY0_test</td><td>0.61046</td></tr><tr><td>Volume_RV</td><td>0.41976</td></tr><tr><td>Volume_RV_test</td><td>0.42339</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.25786</td></tr><tr><td>epoch_time_sec</td><td>14.97754</td></tr><tr><td>train_step_loss</td><td>0.21045</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_BasicUNet_Cross_validation_run_5</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/vwi24cdd' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/vwi24cdd</a><br/> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_131724-vwi24cdd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ENTIRE TRAINING AND MODEL SAVING LOOP, sending all training, validation and test data to wandb\n",
    "\n",
    "\n",
    "experiment_name = \"PreProc_BasicUNet_Cross_validation\" # CHANGE THIS PER RUN!!!!!!!\n",
    "# Define folder based on the WandB run name and create the folder.\n",
    "folder_save_path = experiment_name +  \"_models\"\n",
    "os.makedirs(folder_save_path, exist_ok=True)\n",
    "\n",
    "print(\"Beginning the loop\")\n",
    "# Recombine dataset\n",
    "recombine_index = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Beginning Model train loop\n",
    "for idx in recombine_index:\n",
    "    print(\"Beginning loop for index:\" + str(idx))\n",
    "    # Recombine new validation and training set (in folders)\n",
    "    print(\"Recombining Data\")\n",
    "    recombining_data(idx)\n",
    "    # Start a new wandb run to track this script.\n",
    "    # --------------------------------------------------------------------------------------------------------------------\n",
    "    # Initialize a new WandB run with configuration based on the experiment name.\n",
    "    run = wandb.init(\n",
    "        entity=\"DLMI_Project\",\n",
    "        project=\"DLMI_Project\",\n",
    "        config={\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"architecture\": experiment_name,  # Using experiment name as the architecture identifier\n",
    "            \"dataset\": \"ACDC\",\n",
    "            \"epochs\": 10,\n",
    "        },\n",
    "        name=f\"{experiment_name}_run_{idx}\"\n",
    "    )\n",
    "\n",
    "    # Combine the folder path with the model filename.\n",
    "    model_save_path = os.path.join(folder_save_path, f\"{experiment_name}_cross_variant_{idx}.pth\")\n",
    "    \n",
    "    \n",
    "    # load training and test set\n",
    "\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "\n",
    "    # Define a common preprocessing pipeline\n",
    "    common_transform = Compose([\n",
    "        LoadHeartData(),  \n",
    "        AddChanneld(keys=[\"img\", \"mask\"]), # Add channel dimension\n",
    "        ScaleIntensityd(keys=[\"img\"], minv=0, maxv=1),  # Normalize intensity\n",
    "        Spacingd(keys=[\"img\", \"mask\"], pixdim=(1.25, 1.25), mode=(\"bilinear\", \"nearest\")), # Resample voxel spacing in x and y\n",
    "        # Resized(keys=[\"img\", \"mask\"], spatial_size=(256, 256), mode=(\"area\", \"nearest\")),\n",
    "        ResizeWithPadOrCropd(keys=[\"img\", \"mask\"], spatial_size=[256, 256]), # Ensures all images have the same dimensions (without getting stretched out). \n",
    "        EnsureTyped(keys=[\"img\", \"mask\"])\n",
    "    ])\n",
    "\n",
    "    # Train Transform\n",
    "    train_transforms = Compose([\n",
    "        *common_transform.transforms,  # Apply all common steps first\n",
    "        RandZoomd(keys=[\"img\", \"mask\"], prob=0.1, min_zoom=0.9, max_zoom=1.1, keep_size=True), # Random zoom, not too much so that you don't remove important parts\n",
    "        RandFlipd(keys=[\"img\", \"mask\"], prob=0.1, spatial_axis=0),  # Random flip. Spatial axis=0 for up-down flipping. Left-right flipping is not good because the model has to distinguish the left and right ventricle\n",
    "        RandRotated(keys=[\"img\", \"mask\"], range_x=np.pi/12, prob=0.1, mode=(\"bilinear\", \"nearest\")),\n",
    "        # RandShiftIntensityd(keys=[\"img\"], offsets=0.05, prob=0.5) # Not too much, so that left-right orientation is no problem\n",
    "        # monai.transforms.RandSpatialCropd(keys=['img', 'mask'], roi_size=[256,256], random_size=False),  # Random crop\n",
    "        # monai.transforms.RandShiftIntensityd(keys=['img'], offsets=0.05, prob=0.5),  \n",
    "    ])\n",
    "\n",
    "    test_transforms = common_transform\n",
    "    valid_transforms = common_transform\n",
    "\n",
    "    # validation_test_transforms = monai.transforms.Compose([\n",
    "    #     LoadHeartData(),  # Load the heart data (must be first!)\n",
    "    #     monai.transforms.AddChanneld(keys=['img', 'mask']),  # Add channel dimension for img and multilabel mask\n",
    "    #     monai.transforms.ScaleIntensityd(keys=['img'], minv=0, maxv=1),  # Normalize intensity to the range [0, 1]\n",
    "    #     monai.transforms.Spacingd(keys=[\"img\", \"mask\"], pixdim=(1.25, 1.25), mode=(\"bilinear\", \"nearest\")),  # Resample voxel spacing in x and y\n",
    "    #     monai.transforms.ResizeWithPadOrCropd(keys=['img', 'mask'], spatial_size=[256, 256]),  # Ensure consistent size [256, 256]\n",
    "    # ])\n",
    "\n",
    "\n",
    "    # original training data\n",
    "    train_data = build_dict_acdc(data_path_train, mode='train')\n",
    "\n",
    "    # Create CacheDatasets for training and testing\n",
    "    train_dataset = CacheDataset(\n",
    "        data=train_data,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "\n",
    "    test_dataset = CacheDataset(\n",
    "        data=build_dict_acdc(data_path_test, mode='test'),\n",
    "        transform=test_transforms\n",
    "    )\n",
    "\n",
    "    # compose the 4 training datasets into one variable\n",
    "    valid_dataset = CacheDataset(\n",
    "        data=build_dict_acdc(data_path_valid, mode='val'),\n",
    "        transform=test_transforms\n",
    "    )\n",
    "\n",
    "    # Construct CacheDataset from the list of dictionaries and apply the transform\n",
    "    #train_dataset = monai.data.CacheDataset(data=train_dict_list, transform=LoadHeartData())\n",
    "    #test_dataset = monai.data.CacheDataset(data=test_dict_list, transform=LoadHeartData())\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "    # validation_loader\n",
    "\n",
    "\n",
    "\n",
    "    # DEFINE THE ARCHITECTURE\n",
    "    # ---------------------------------------------------------------------------------------------------------------\n",
    "    # Define the device to use\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "    print(\"CUDA Current Device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No GPU\")\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Initialize the U-Net model.\n",
    "    # Here, dimensions=2 for 2D slices; in_channels=1 and out_channels=1 for binary segmentation.\n",
    "    model = UNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "    ).to(device)\n",
    "\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    # Define the loss function and optimizer.\n",
    "    # DiceLoss with sigmoid=True is used for binary segmentation.\n",
    "    loss_function = DiceLoss(softmax=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "    # (Optional) DiceMetric for evaluation during training\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "\n",
    "    class_labels = {\n",
    "        0: \"Background\",\n",
    "        1: \"Right Ventricular Endocardium\",\n",
    "        2: \"Left Ventricular Epicardium\",\n",
    "        3: \"Left Ventricular Endocardium\"\n",
    "    }\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "\n",
    "    print(torch.__version__)\n",
    "\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    #unique_values = torch.unique(labels)\n",
    "    #print(\"Unique values in batch labels:\", unique_values)\n",
    "\n",
    "    # START THE TRAINING\n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    # Training loop\n",
    "    num_epochs = wandb.config.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 10, f\"Epoch {epoch + 1}/{num_epochs}\", \"-\" * 10)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = batch_data[\"img\"].to(device)\n",
    "            # Convert labels from shape (B, 1, H, W) to (B, H, W)\n",
    "            labels = batch_data[\"mask\"].squeeze(1).to(device)\n",
    "            unique_values = torch.unique(labels)\n",
    "            #print(\"Unique label values:\", unique_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # shape: (B, 4, H, W)\n",
    "            outputs = outputs.contiguous()\n",
    "            # Convert labels to one-hot encoding: shape becomes (B, H, W, 4)\n",
    "            one_hot_labels = F.one_hot(labels.long(), num_classes=4)\n",
    "            # Permute to get shape (B, 4, H, W)\n",
    "            one_hot_labels = one_hot_labels.permute(0, 3, 1, 2).float()\n",
    "\n",
    "            loss = loss_function(outputs, one_hot_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            #print(f\"{step}/{len(train_loader)}: loss = {loss.item():.4f}\")\n",
    "            wandb.log({\"train_step_loss\": loss.item(), \"epoch\": epoch + 1})\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}, time: {epoch_time:.2f} sec\")\n",
    "\n",
    "        # Evaluate on the test set at the end of each epoch\n",
    "        voxel_size = (1.25, 1.25)  # For 2D slices; adjust as needed\n",
    "\n",
    "        model.eval()\n",
    "        all_metrics = []\n",
    "        with torch.no_grad():\n",
    "            for val_data in valid_loader: # switch to validation loader\n",
    "                val_inputs = val_data[\"img\"].to(device)\n",
    "                val_labels = val_data[\"mask\"].squeeze(1).to(device)  # shape: (B, H, W)\n",
    "                val_outputs = model(val_inputs)  # shape: (B, 4, H, W)\n",
    "\n",
    "                # For evaluation, use the integer label maps directly.\n",
    "                pred_labels = torch.argmax(torch.softmax(val_outputs, dim=1), dim=1)  # (B, H, W)\n",
    "                gt_labels = val_labels  # already in (B, H, W) after squeeze\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                pred_labels_np = pred_labels.cpu().numpy()\n",
    "                gt_labels_np = gt_labels.cpu().numpy()\n",
    "\n",
    "                for gt, pred in zip(gt_labels_np, pred_labels_np):\n",
    "                    sample_metrics = metrics(gt, pred, voxel_size)\n",
    "                    all_metrics.append(sample_metrics)\n",
    "\n",
    "            avg_metrics = np.mean(all_metrics, axis=0)\n",
    "            print(\"Validation metrics:\", avg_metrics)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"epoch_loss\": epoch_loss,\n",
    "                \"Dice_LV\": avg_metrics[0],\n",
    "                \"Volume_LV\": avg_metrics[1],\n",
    "                \"Err_LV\": avg_metrics[2],\n",
    "                \"Dice_RV\": avg_metrics[3],\n",
    "                \"Volume_RV\": avg_metrics[4],\n",
    "                \"Err_RV\": avg_metrics[5],\n",
    "                \"Dice_MY0\": avg_metrics[6],\n",
    "                \"Volume_MY0\": avg_metrics[7],\n",
    "                \"Err_MY0\": avg_metrics[8],\n",
    "                \"epoch_time_sec\": epoch_time\n",
    "            })\n",
    "    \n",
    "\n",
    "    # Save the trained model at the end\n",
    "\n",
    "    # Save the model.\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "    \n",
    "    # Get test results\n",
    "    model.eval()\n",
    "    all_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for val_data in test_loader: # switch to validation loader\n",
    "            val_inputs = val_data[\"img\"].to(device)\n",
    "            val_labels = val_data[\"mask\"].squeeze(1).to(device)  # shape: (B, H, W)\n",
    "            val_outputs = model(val_inputs)  # shape: (B, 4, H, W)\n",
    "\n",
    "            # For evaluation, use the integer label maps directly.\n",
    "            pred_labels = torch.argmax(torch.softmax(val_outputs, dim=1), dim=1)  # (B, H, W)\n",
    "            gt_labels = val_labels  # already in (B, H, W) after squeeze\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            pred_labels_np = pred_labels.cpu().numpy()\n",
    "            gt_labels_np = gt_labels.cpu().numpy()\n",
    "\n",
    "            for gt, pred in zip(gt_labels_np, pred_labels_np):\n",
    "                sample_metrics = metrics(gt, pred, voxel_size)\n",
    "                all_metrics.append(sample_metrics)\n",
    "\n",
    "        avg_metrics = np.mean(all_metrics, axis=0)\n",
    "        print(\"Validation metrics:\", avg_metrics)\n",
    "        wandb.log({\n",
    "            \"Dice_LV_test\": avg_metrics[0],\n",
    "            \"Volume_LV_test\": avg_metrics[1],\n",
    "            \"Err_LV_test\": avg_metrics[2],\n",
    "            \"Dice_RV_test\": avg_metrics[3],\n",
    "            \"Volume_RV_test\": avg_metrics[4],\n",
    "            \"Err_RV_test\": avg_metrics[5],\n",
    "            \"Dice_MY0_test\": avg_metrics[6],\n",
    "            \"Volume_MY0_test\": avg_metrics[7],\n",
    "            \"Err_MY0_test\": avg_metrics[8]\n",
    "        })\n",
    "        \n",
    "        print(\"Test metrics:\", avg_metrics)\n",
    "    \n",
    "    run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (new env)",
   "language": "python",
   "name": "python3_11_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
